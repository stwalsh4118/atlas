# [3-6] Implement progress logging

[Back to task list](./tasks.md)

## Description

Add comprehensive logging to the import pipeline to track progress, performance metrics, and errors. This will provide visibility into long-running imports and help troubleshoot issues when they occur.

## Status History

| Timestamp | Event Type | From Status | To Status | Details | User |
|-----------|------------|-------------|-----------|---------|------|
| 2025-10-20 12:00:00 | Created | N/A | Proposed | Task file created | AI_Agent |

## Requirements

1. Log import start time and end time
2. Log record count every 1000 records during import
3. Calculate and display records per second
4. Log total records imported
5. Log total import duration
6. Create structured log file with timestamp
7. Display progress to stdout during import
8. Log errors with context (which record, what error)
9. Include configuration details in log (source file, CRS, etc.)
10. Support different log levels (INFO, WARN, ERROR)

## Implementation Plan

1. Create logging utility functions in import script
2. Add timestamp function: `log_timestamp()`
3. Add logging functions:
   - `log_info(message)`
   - `log_warn(message)`
   - `log_error(message)`
4. Create log directory: `logs/`
5. Generate log filename with timestamp: `logs/import-YYYYMMDD-HHMMSS.log`
6. Add tee command to send output to both stdout and log file
7. Add progress monitoring for ogr2ogr with -progress flag:
   ```bash
   ogr2ogr -progress ...
   ```
8. Log pre-import configuration:
   - Source GeoJSON file path
   - File size
   - Source CRS
   - Target CRS
   - Field mapping file
   - Record count estimate
9. Log post-import summary:
   - Total records inserted
   - Duration
   - Records per second
   - Any errors encountered
10. Update all scripts to use logging functions

## Test Plan

**Objective**: Verify logging provides useful progress and error information

**Test Scope**: Logging functionality across all scripts

**Key Test Scenarios**:
1. Log file is created with correct timestamp
2. Import progress is logged every 1000 records
3. Start and end times are logged
4. Duration is calculated correctly
5. Records per second metric is accurate
6. Errors are logged with full context
7. Log output is both displayed and saved to file

**Success Criteria**: 
- Structured log files created
- Progress updates appear during import
- Performance metrics are accurate
- Errors are logged with sufficient detail for debugging
- Log format is consistent and parseable

## Verification

- [ ] Log directory created: `logs/`
- [ ] Logging functions added to import script
- [ ] Log files created with timestamp naming
- [ ] Import start/end logged
- [ ] Progress logged every 1000 records
- [ ] Record count displayed
- [ ] Duration calculated and logged
- [ ] Records per second calculated
- [ ] Errors logged with context
- [ ] Configuration details logged
- [ ] Output teed to both stdout and log file
- [ ] Tested with Montgomery County import

## Files Modified

- `scripts/import-parcels.sh` (modified)
- `scripts/validate-geodata.sh` (modified)
- `scripts/validate-geometries.sh` (modified)
- `logs/` (new directory)


